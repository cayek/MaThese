# -*- coding: utf-8 -*-
# -*- mode: org -*-

#+TITLE: Méthodes de factorisation matricielle pour la génomique des populations et les tests d'association
#+AUTHOR:      Kevin Caye

#+LANGUAGE:  en
#+STARTUP: overview indent inlineimages logdrawer
#+OPTIONS: H:5 author:nil email:nil creator:nil timestamp:nil skip:nil toc:nil ^:nil
#+TAGS: noexport(n) deprecated(d)
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport

# #+LaTeX_CLASS: book
#+LaTeX_CLASS: article
#+LATEX_HEADER: \input{notations.tex}

#+HTML_MATHJAX: align: left indent: 5em tagside: left font: Neo-Euler

#  LocalWords:  methylation polymorphism nucleotide Frobenius invertible SNP
#  LocalWords:  preprocessing dataset RidgeLFMM LassoLFMM

#+BEGIN_QUOTE
In Code we trust, all others bring data.
–William Edwards Deming (1900-1993).
#+END_QUOTE

* Introduction
:LOGBOOK:
- Note taken on [2017-06-09 ven. 16:37] \\
  Il faut que j'ai travailler sur deux méthodes ! Les deux répondes à deux
  problématique différentes et le tout s'inscrit dans un besoin t'outils adapté à
  la génétique du 21ieme siecle !!
- Note taken on [2017-06-09 Ven 11:44] \\
  dans l'intro il faut que je motive la problématique !! et le plan répond a cette
  problématique.
:END:
** Contexte
:LOGBOOK:
- Note taken on [2017-06-09 Ven 11:47] \\
  c'est un context de fouille de données trop grosse !! Il faut amener de
  l'information à un niveau inteligible.
- Note taken on [2017-06-05 Mon 10:38] \\
  Ca peut etre cool de replacer le context historique en partant de la niasance
  des stats (fisher etc) et de faire le parallele avec maintenant pour on a
  suffisament de données pour se rendre compte que nos test d'hypothèse sont faux
  :D et la on fait le lien avec les tests d'hypothèe multiple....
:END:

Cette dernière décennie a été marquée par une accumulation des données dans tous les
domaines de la sciences. Cette accumulation de données est une aubaine pour les
scientifiques. Cependant, que faire d'autant de données et comment en tirer
l'information qui permettrait de mieux comprendre le monde qui nous entoure ? Il
s'agit là d'un défi majeur pour les statistiques cite:slides_sfds2015_saporta. 

Les grandes données posent plusieurs problèmes. En effet, si l'on est capable d'obtenir
des données rapidement, on veut pouvoir les analyser rapidement. Cependant de
nombreux modèle statistiques classiques ne passent pas l'échelle des grands jeux
de données. Il est donc nécessaire de repenser les modèles et algorithmes afin
de les adapter au nous volumes des données. 
... parler de l'inversement du processus d'aquisition des données .. cf
seminaire Bosson



Dans le cadre de cette thèse nous nous sommes intéressé a développer des méthodes
statistiques utiles à deux problématique scientifiques. Le premier est l'estimation
de la structure de population à partir de données génomique. Le deuxièmes est les
problèmes des test d'association multiple. Toutes les méthodes statistiques
developper lors de cette thèse repose sur la factorisation de matrice. Nous
allons maintenant introduire plus en détails les problématiques ainsi que la
factorisation de matrice en statistique.


** La génomique des populations
:LOGBOOK:
- Note taken on [2017-06-07 Mer 14:42] \\
  - analyse de la structure de variance covariance: PCA
  - analyse de la structure de population: structure, snmf, etc
  - ewas: refactor
:END:
** Test d'association
** La factorisation de matrice en statistique
** Problématique et plan
* Inférence rapide des coefficients de métissage à l'aide des données géographique
:LOGBOOK:
- Note taken on [2017-06-05 Mon 13:44] \\
  Ce qui serais stylé c'est d'ajouté une cross validation propre pour tess3 :D, et
  de relancer les analyse sur AT, voir pk pas sur les très gros dataset AT :D !!!
  
  On ne toucherais pas à l'autre papier mais on lance sur ce dataset la même
  analyse mais très proprement :D, y compris pour l'étude stat à la fin
  (recalibration propre !)
:END:
** Detection de la structure de population
** Test adaptation local 
** presentation de l'article 1
* Estimation des facteurs latents pour corriger les test d'association :3Article:
** before article 2
*** Les tests d'hypothèse multiple
*** Les modèle a facteur lattents pour ca
*** presentation de l'article 2
** Article
*** Introduction
*** Methods
**** Model 
We present here the model and the notations which will be used in the article.
Following the common notation in latent factor mixed model (LFMM) we write the following
model
\begin{equation}
\label{eq:model}
\Y = \X \B^T + \U \V^T + \E 
\end{equation}
Here $\Y$ is a $\Yrow\times\Ycol$ observed output matrix. For instance, output can
be single nucleotide polymorphism (SNP) or methylation level. The $\Yrow\times\Xcol$
matrix $\X$ records observed primary variables. Primary variables can be for
example disease state or environmental gradient. 
The $\Ycol\times\Xcol$ matrix $\B$
records primary effects. 
Matrices $\U$ and $\V$ are respectively $\Urow \times \K$
score matrix and $\Ycol\times\Ucol$ loading matrix of a regression with $\Ucol$ latent
factors. The matrix $\U$ record the unobserved latent variables.
The matrix $\E$ is the residual error matrix of size $\Yrow\times\Ycol$.

A classic method to estimate $\U$ $\V$ and $\B$ is to write the following
loss function
\begin{equation}
\label{eq:optim_no_reg}
\LfmmL 
\end{equation}
where $\norm{.}_{F}$ is the Frobenius norm. This loss function arises when
considering the log-likelihood if $\E$ rows are independent and Gaussian 
with means $0$ and $\Var(\E_{i,j}) = \sigma$.

However, the function $\Lnoreg$ do not allow to define unique latent factor
matrices product $$\C = \U \V^T$$ and a unique primary effect matrix $\B$. It
means that we can not identify the output variance explained by the $K$ latent
factors and the output variance explained by $\X$.

We will now expose two regularized variants of function $\Lnoreg$ which lead to
unique primary effect matrix $\B$ and latent factor matrices product $\C$.

***** demo
We write $\hat{\B}$ the unique matrices of optimization problem defined by the
loss function $\L$ in [[eqref:eq:optim_no_reg]], and $\hat{\U}$, $\hat{\V}$
possible matrices for this solution. First, we remark that $\hat{\U}$ and
$\hat{\V}$ are not unique because, for any invertible matrix $\matr{R}$ we have
$$L(\hat{\U}, \hat{\V}^{T}, \hat{\B}) = L(\hat{\U} \matr{R}, \matr{R}^{-1}
\V^{T}, \hat{\B}).$$ Then, we remark that for a any $\Xcol \times \Ycol$ matrix
$\matr{C}$
\begin{equation*}
L(\hat{\U} - \X \matr{C}, \hat{\V}^{T}, \hat{\B} + \hat{\V} \matr{C}^T}) = L(\hat{\U},
\hat{\V}^{T}, \hat{\B})
\end{equation*}
Thus, $\hat{\B}$ is not unique.


**** Ridge regularized latent factor mixed model (RidgeLFMM)
In this section we present a ridge regularized latent factor mixed model
(RidgeLFMM). The regularized loss function is written as follow 
\begin{equation}
\label{eq:optim_ridge_reg}
\LfmmLridge
\end{equation}
We can show that minimizing the function eqref:eq:optim_ridge_reg leads to the
following unique solution
\begin{align}
\label{eq:RidgeLfmmEstomatorC}
\hat{\U} \hat{\V} & =  \sqrt{\obP}^{-1} \mathrm{svd}_{\K}(\sqrt{\obP} \Y ) \\
\hat{\B} & = (\X^{T} \X + \lambda \Id_{d})^{-1} \X^{T} (G - \hat{\U} \hat{\V}).
\end{align}
Where $svd_{\K}(\matr{A})$ is the best approximation of rank $\K$ for the matrix
$\matr{A}$ given by the singular value decomposition (SVD). $\Id_{q}$ is the $q \times
q$ identity matrix. The $\Yrow \times \Yrow$ matrix $\obP$ is an oblique
projection matrix defined as $$\LfmmP.$$ The matrix $\sqrt{\obP}$ is define such that 
\begin{equation*}
\obP = \sqrt{\obP}^{2}
\end{equation*}


***** Proof
We want to find 
\begin{align*}
\hat{\U} & \in \RR^{\Urow \times \Ucol} \\
\hat{\V} & \in \RR^{\Vrow \times \Vcol} \\
\hat{\B} & \in \RR^{\Brow \times \Bcol} \\
\end{align*}
which minimize the function $\Lridge$ define in eqref:eq:optim_ridge_reg. We
start to compute the Jacobian of $\Lridge$ along $\B$, $\U$ and $\V$
\begin{equation}
\begin{cases}
\label{eq:partial_Lridge}
& \frac{\partial \Lridge}{\partial \B}(\U, \V, \B) = \X^{T} (\U \V^{T} + \X \B^{T} - \Y) + \lambda \Id_{d} \B^{T} \\
& \frac{\partial \Lridge}{\partial \V}(\U, \V, \B) = \U^{T} (\U \V^{T} + \X \B^{T} - \Y) \\
& \frac{\partial \Lridge}{\partial \U}(\U, \V, \B) = (\U \V^{T} + \X \B^{T} - \Y) \V

\end{cases}
\end{equation}
The minimum is reach if and even if  eqref:eq:partial_Lridge equal to
zero. We can write
\begin{equation}
\begin{cases}
\label{eq:euler_Lridge}
& \hat{\B}^{T} = (\X^{T} \X + \lambda \Id_{\Bcol})^{-1} \X^{T} (\Y - \U \V) \\
& 0 = \U^{T} (\U \V^{T} + \X \B^{T} - \Y) \\
& 0 = (\U \V^{T} + \X \B^{T} - \Y) \V
\end{cases}
\end{equation}
Then by using the first line of eqref:eq:euler_Lridge the two last ones we have
\begin{equation}
\label{eq:euler_UV_Lridge}
\begin{cases}
&  0 = \hat(\U)^{T} \obP (\hat{\U} \hat{\V}^{T} - \Y) \\
& 0 = \obP (\hat{\U} \hat{\V}^{T} - \Y) \hat{\V}
\end{cases}
\end{equation}
Finally, finding the solutions $\hat{\U}$ and $\hat{\V}$ of
eqref:eq:euler_UV_Lridge is equivalent to find the minimum of 
\begin{equation}
\mathrm{L^{'}_{ridge}}(\U, \V) = \frac{1}{2} \norm{ \sqrt{\obP} (\Y - \U \V^{T})}_{F}^{2}
\end{equation}
which is the classic problem of finding $\K$ rank best approximation of the matrix
$$ \sqrt{\obP} \Y.$$
As result we have 
\begin{align*}
\hat{\U} \hat{\V} & =  \sqrt{\obP}^{-1} * \mathrm{svd}_{\K}(\sqrt{\obP} \Y ) \\
\hat{\B} & = (\X^{T} \X + \lambda \Id_{d})^{-1} \X^{T} (G - \hat{\U} \hat{\V}).
\end{align*}

**** Lasso regularized latent factor mixed model (LassoLFMM)
In this section we present a lasso regularized latent factor mixed model (LassoLFMM)
The regularized loss function is written as follow
\begin{equation}
\label{eq:optim_lasso_reg}
\LfmmLlasso
\end{equation}
Where $\norm{.}_{*}$ is the nuclear norm. Contrary to $\Lridge$ finding minimum
solution of $\Llasso$ is not easy. However, if we made the variable change $$ \C = \U \V^{T}
$$ in eqref:eq:optim_lasso_reg, the function $\Llasso$ become a convex
function of $\C$ and $\B$. Thereby, we can apply alternated algorithm to compute
minimum value of $\Llasso$.

***** An alternated algorithm
<<sec:lasso_algo>>

We start with null matrices
\begin{align*}
C_{t = 0} & = 0 \\
B_{t = 0} & = 0.
\end{align*}
Then we alternate the two steps 
- compute $\C_{t}$ as minimizing the loss function
\begin{equation}
\mathrm{L_{lasso}^{1}}(\C) = \frac{1}{2} ||(\Y - \X \B_t^T)- \C ||_{F}^2 + \gamma ||\C||_{*}.
\end{equation}
- compute $\B_{t}$ as minimizing the loss function
\begin{equation}
\mathrm{L_{lasso}^{2}}(\B) =  \frac{1}{2} ||(\Y - \C_{t-1}) - \X \B^T||_{F}^2 + \lambda ||\B||_1
\end{equation}

The first step is a low rank approximation of the
regression residual matrix 
$$\matr{E}^{1} = \Y - \X \B_{t}^{T}.$$ 
which is given by the singular value shrinkage operator
cite:cai10_singul_value_thres_algor_matrix_compl. 
If we write the singular value
decomposition of $\matr{E}^{1}$ as follow
$$
\matr{M} \Sigma \matr{N}^{T}
$$
were $\matr{M}$ and $\matr{N}$ are respectively $\Yrow \times \K$ and $\Ycol
\times \K$ matrices of orthogonal columns and 
$$
\Sigma = \mathrm{diag}(\{\sigma_{i}\}_{1 \leq i \leq \K})
$$
the diagonal matrix of singular values. Then
the singular value shrinkage operator of $\matr{E}^{1}$ gives
$$
\matr{M} \Sigma_{\gamma} \matr{N}^{T}
$$
where 
$$ \Sigma_{\gamma} = \mathrm{diag}(\{(\sigma_{i} - \gamma)_{+}\}_{1 \leq i \leq \K}) $$ 
and 
$$t_{+} = \mathrm{max}(0, t).$$

The second step is a regression of the residual matrix $$\matr{E}^{2} = \Y -
\C_{t-1}$$ by the primary variable $\X$ with a lasso regularization on the
primary effect matrix $\B$.

We iterate these two steps until the algorithm converges to the minimum of
$\Llasso$ defined in eqref:eq:optim_lasso_reg.

**** Choice of hyper-parameters
:LOGBOOK:
- Note taken on [2017-05-25 Thu 11:52] \\
  Pour ridge faire ma petite heuristic pour trouver lambda.
  Pour lasso aussi (chemin de reg).
- Note taken on [2017-05-25 Thu 11:49] \\
  Pour une estimation precise des parametre il y a la cross validation. Sinon
  comme la méthode resemble a l'acp auquel on a enlevé la variance expliqué par X
  on peut utiliser les même éthodes que pour l'acp. Quite à surestimer le nombre
  de facteur lattent.
- Note taken on [2017-05-25 Thu 11:46] \\
  Bien preciser que on veut a tou pris eviter les truc du style j'impute a
  l'arrache avant etc...
:END:

LassoLFMM and RidgeLFMM method results depend on two kinds of hyper-parameter,
the number of latent factors and the regularization parameters. We present here
practice solutions to assess these hyper-parameters. 

***** Cross validation
:LOGBOOK:
- Note taken on [2017-05-26 Fri 14:46] \\
  cf mon cahier
:END:
Cross validation is a classic method to select hyper-parameter in factor
analysis cite:Owen_2009,Bro_2008. The cross validation algorithm we used is
explained in detail in annex. Cross validation procedure can be long to run in
particular on very big data set. Especially since LassoLFMM and RidgeLFMM have
each two hyper-parameters which can be cross-validated. We propose other procedure
to assess hyper-parameters that gave good results in our experiments.

****** COMMENT ANNEX Cross validation algorithm
We present here the cross validation algorithm which can be used to assess
hyper-parameter of LassoLFMM and RidgeLFMM.

We first split the observation output matrix into 
$$
\Y^{(-I)}
$$
and 
$$
\Y^{(I)}
$$ where matrices are respectively compose of 


. We write the training
data matrices $$ \Y^{(-i)}$$ and $$\X^{(-i)}.$$ These matrices are compose of
random lines of the observed output matrix and the primary matrix. Then we
compute (with RidgeLFMM or LassoLFMM) latent factor matrices $$\U^{(-i)}$$ and
$$\V^{(-i)}$$ and the primary effect matrix $$\B^{(-i)}.$$

In a second step we want to compute an estimation of test output matrix
$$\Y^{(i)}$$. However, to predict the output matrix with LFMM model
eqref:eq:model we must estimate the latent score matrix 
$$\U^{(i)}$$ 
for this the test data.
To avoid using test observation for the prediction, we split the test data
by selecting a random set of variables of the observed output matrix which we
note $$\Y^{(i)_{(-j}}$$. 

Then, if we assume that we know the primary effect size
matrix and the latent factor loading matrix for staying variables we write 

\begin{equation}
\label{eq:cvU}
U^{(i)}_{(-j)} = (\Y^{i}_{(-j)} - \X^{(i)}} \B^{(-j)}) \V_{(-j)}^{T}.
\end{equation}

The equation eqref:eq:cvU is given by optimal solution of $L$
ref:eq:optim_no_reg when $V$ and $B$ are fixed.

Finally, we compute the residual error on the test set such as 
\begin{equation}
Err = \norm{ \Y^{i}_{(j)} + \X^{(i)}} \B^{(-i)_{(j)} + \U^{(i)}_{(-j)} \V^{(-i)}
\end{equation}

***** Choice of K using singular value
Methods presented in this paper are very close to the Principal Component
Analysis (PCA), we can seen them as a PCA of $$\Y - \X \B^{T}$$ if we know the
primary effect matrix $\B$. Thus we propose to select the number of latent variables
$\K$ by visualizing the scree plot.

We empirically observed that this method leads to an overestimated number of
factor in the model described in eqref:eq:model since the co-variate would be
considered as a latent variable. However, because the goal of our methods is to
estimate latent variation while protecting variation explained by co-variate $\X$,
we observed that our algorithms was robust to overestimated $\K$.

***** Heuristic to choice of $\lambda$ for RidgeLFMM
:LOGBOOK:
- Note taken on [2017-06-01 jeu. 12:03] \\
  et la on fait le lien avec le model de cate :D
- Note taken on [2017-05-26 Fri 14:45] \\
  voir mon cahier (30/01/2017) et il va falloir normaliser lambda ?? a voir !!C'est chiant car
  j'ai deja lancé les experiences !!
:END:
In our experiments, we remarked that low values of $\lambda$ gave better
results. We try here to give insights to understood this remark.

Firstly we write the oblique projection of model equation eqref:eq:model as follow
\begin{equation}
\sqrt{\obP} \Y = \sqrt{\obP} (\U \V^{T} + \E)+ \sqrt{\obP} X \B^{T}.
\end{equation}
When $\lambda$ is close to zero the term 
\begin{equation}
\label{eq:obPXB}
\sqrt{\obP} X \B^{T}
\end{equation}
tends to zero. Thus if the latent score stored in $\U$ was not correlated with
$\X$ we must chose $\lambda$ equal zero and compute latent factors and primary
effects separately. As identified in cite:wang2015confounder, the complicated
case is when latent variables $\U$ and primary variables $\X$ are correlated. In
this case we want $\lambda$ to be small enough such that eqref:eq:obPXB tends
to zero and the SVD in ref:eq:RidgeLfmmEstomatorC to extract only the
latent variables. But if $\lambda$ is to close to zero the latent variables
should be hard to estimate with SVD in particular if there are highly correlated
with $\X$ and the number of output variables correlated with X is high.

We observed that for a centered and normalized output and primary matrices $\Y$
and $\X$, $\lambda = \frac{1}{\Yrow} 10^{-5}$ provided good results in our
experiments.

****** COMMENT cate model
In the article of \MethodCate method cite:wang2015confounder, authors propose to
explicitly model the relationship between the factor score matrix $\U$ and the
primary variables matrix $\X$. They assume that there is a linear relationship
between $\U$ and $\X$ such as 
\begin{equation}
\label{eq:CorUX}
\U = \X \matr{\alpha}^{T} + \matr{W},
\end{equation}
where
$\W$ is a $\Urow \times \K$ residual error matrix independent of $\X$ and $\E$
and $matr{\alpha}$ $\Xcol \times \Ucol$ characterizes the linear relationship
between $\U$ and $\X$. If $\matr{\alpha}$ is null, there is no problem of
confounding and $\U$, $\V$ and $\X$ can be estimated separately.  

***** Heuristic to choice of $\gamma$ for LassoLFMM
This hyper-parameter impact the rank of the $C$ matrix. To assess the gamma
value we compute singular values of $\Y$ $(\sigma_1, ..., \sigma_{\Yrow})$. Then
we set
$$
\gamma = \frac{(\sigma_{\K} + \sigma_{\K + 1})}{2} 
$$
for $K$ the chosen number of latent factors. In our experiments, we observed
that for such computed $\gamma$ the rank of $\C$ returned by lasso algorithm was
$K$.


***** Heuristic to choice of $\lambda$ for LassoLFMM
:LOGBOOK:
- Note taken on [2017-06-13 Mar 15:21] \\
  changer dans le papier et dans le code + introduire la svd soft dans le papier !!
- Note taken on [2017-06-13 Mar 15:20] \\
  ce qui est implementé c'est reg de G - C avec la svd du couo il faut que je
  commence par la svd pas par la reg linear !!!
:END:
This hyper-parameter impact the number of line set to zero in $B$. We know that
only a part of observe variable $G_j$ are correlated with the variable $X$. So
we can interpret the proportion on non zero line in $B$ as the proportion of
variables which correlate with $X$. To find the lambda which correspond to the
proportion we propose an heuristic based on a regularization path of lambda
value inspired by cite:friedman10_regul_paths_gener_linear_model. 

We start with the smallest value of $\lambda$ such that the entire vector $$
\hat{\B} = \mathrm{argmin} \norm{(\Y - \C_{t = 1}) - \X \B^{T}} + \lambda |B|$$ equal
zero, here $$\Y - \C_{t = 1}$$ is residual of the singular value shrinkage
operator. This is the result of the lasso algorithm first step explain in
section [[sec:lasso_algo]].

Then we construct a sequence of m values of $\lambda$ decreasing from
$\lambda_{\mathrm{max}}$ to $\lambda_{\mathrm{min}}$ on the log scale. Typical
values are \epsilon = 0.001 and K = 100. The algorithm is run for decreasing
value of this sequence. Each time the algorithm converges for a $\lambda$ we
compute the number of non zero line in the computed $\B$ and stop if the
interested proportion of non zero in $\B$ is reached. Otherwise we continue with
the following value of $\lambda$ is the sequence.

**** Hypothesis testing
:LOGBOOK:
- Note taken on [2017-05-25 Thu 11:55] \\
  parler de lm : G ~U + X 
  ET
  la recalibration par mad + median
:END:

Until now, we only presented how to estimate latent factor and primary effect
matrices. However, the purpose which motivate the estimation of latent factor by
considering the primary variable is the detection of observed output variables
associated with the variable of interest. We propose here to use latent factor
score matrix $\U$ as co-variables of a linear model.

***** Linear model with latent factor score
:LOGBOOK:
- Note taken on [2017-05-26 Fri 15:35] \\
  faut que je choississe les notations mieux que ca, je m'enmmèle la ...
:END:

After computing latent factors score matrices $\U$ with the lasso or ridge
algorithm, we use them as co-variables with $\X$ in a linear model. For each
observed output $\Y_{j}$ we have
\begin{equation}
\Y_{j} =  \U \matr{\gamma}_{j}^{T} + \X \beta_{j} + \matr{\epsilon_{j}}
\end{equation}
where $\matr{\epsilon_{j}}$ is a Gaussian error with mean zero.
Then, we can compute the p-value to test the null hypothesis 
$$
\beta_j = 0.
$$


***** Hypothesis calibration
:LOGBOOK:
- Note taken on [2017-06-01 jeu. 14:45] \\
  Voir dans cite:gerard2017unifying la parti sur la calibration !
:END:

Even with latent factors correction we can observed not calibrated p-value. This
can be due to presence of not interested but small effects, correlation between
observations or remaining unobserved variables cite:Efron_2004. 

In true data analysis we expect that a small proportion of output variables will
be associated with the primary variables (not more than $1 \%$). Thus we used
empirical calibration for the variance and the mean of statistical test. In
particular, as in cite:wang2015confounder,gerard2017unifying, we used the median
and the mad as robust estimators of the mean and standard deviation of
statistical test under the null hypothesis.

*** Simulation study and dataset
:PROPERTIES:
:header-args: :cache no :eval no-export :results output :exports none
:END:
**** Others methods
<<sec:similar_method>>
***** lm and lm + pca
We comparared results of our method to two well known method the linear model
and the linear model with PCA scores. 
***** cate

***** sva
***** famt
**** Simulations and data

***** Generative model simulation

We used equation to generate generative model dataset. The latent factor
scores and loadings $U$ and $V$ were generated using a multivariate gaussian
distribution with a zero mean and a $K$ identity matrix for the covariance
matrix where is the number of latent factor. The error matrix $E$ was
generated using a multivariate gaussian distribution with a zero mean and a
$L$ identity matrix for the covariance matrix where $L$ is the number of
variables. The co-variable $X$ was generated with a normal distribution with
the mean equal to zero and the standard deviation equal to one such that the
Pearson linear correlation between $X$ and $U_1$ the first latent score
matrix equal to $c$.

***** Real data example
In this section we present the real data we used to compare lasso LFMM, ridge
LFMM with similar methods presented in section [[sec:similar_method]]. To evaluate
the utility of our methods on several situation we select study where correction
for confounding variables is an important step. We realized genome wide
association study (GWAS), an genome-wide association study (EWAS) and an
ecological association study (EAS). Before running algorithm $\G$ and $\X$
matrix was centered and normalized with standard deviation for all the study. We
now describe preprocessing step for each study.

****** Association study of DNA methylation with rheumatoid arthritis (EWAS)
For the EWAS we chose data from a recent association study of DNA methylation with
rheumatoid arthritis (RA) cite:Liu_2013. We retrieve the RA data from Gene
Expression Omnibus (GEO) database (accession number GSE42861). Following
cite:Zou_2014 we filtered out site if its average probe $\beta$ value was above
0.8 are below 0.2. We finally obtain $n = 689$ and $L = 162038$.

#+BEGIN_SRC R :session *ssh krakenator*
  G <- readRDS("~/Projects/Thesis/Data/ThesisDataset/3Article/GSE42861/G.rds")
  dim(G)
#+END_SRC

#+RESULTS:
: [1]    689 162038

For this data set confounding variables (batch effect, age, gender, smoking
status, cell-type composition) are known but we did not use them in methods.
Thus, we can compare methods output with output of method considering explicitly
these variables cite:Rahmani_2016,Zou_2014.

****** Association study of genetic variants with Celiac disease (GWAS)
For the GWAS we chose data from an association study of SNPs with Celiac disease
citep:dubois2010multiple. Before running method we apply classic preprossessing
step with the software Plink cite:Purcell_2007. Firstly, we keep only individual
and SNPs with a proportion of missing value inferior to $5\%$. Then, we filter
out variants with minor allele frequency below $0.05$ and Hardy-Weinberg
equilibrium exact test \pvalue below $1e-10$. After that we filter out
individuals which have identity-by-descent proportion (first by pairs) superior
to $0.08$. Finally, we perform an linkage disequilibrium pruning to obtain SNPs
which are not correlated. The final dataset was of size $n = $ and $L = $.

#+BEGIN_SRC R :session *ssh krakenator*
  G <- readRDS("~/Projects/Thesis/Data/ThesisDataset/3Article/Celiac/G_clumped.rds")
  dim(G)
#+END_SRC

#+RESULTS:
#+begin_example
[1] 15155 94497
#+end_example

We also impute missing value with the sowtware 

****** Association study of genetic variants with climatic data (EAS)
For EAS
*** Results
*** Discussion
*** Figures and tables
**** Numerical experiments
**** GWAS
**** EWAS
**** EAS
bibliographystyle:unsrt
bibliography:../biblio.bib


* Conclusion 


* COMMENT perspectives
:LOGBOOK:
- Note taken on [2017-05-26 Fri 15:49] \\
  Je pense que je ne vais pas pouvoir développer la crossvalidation et les données
  manquante. 
  
  Par contre je peux montrer que si la cross validation est mal faite
  ca abouti a des mauvais choix de parametre (exemple)
  
  Pareil pour les données manquantes. 
  
  Après dans mes application il n'y a jamais trop de données manquantes, donc peut
  être que c'est pas la peine de se prendre la tête... Surtout que la cross
  validation j'en aurai deja parlé !
:END:


